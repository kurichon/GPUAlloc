**Training Environment Name** (using conda): py3.8.16
- conda activate py3.8.16
- CUDA 11.2
- Cuda and CUDNN files in /Installation
  
**Directories:**
- /networks contains the generative networks
- CNN models are found in /tf_cnn_benchmarks
- /driple contains all the scripts used for training
- /data contains all the collected data for the research
  
**Job Profiling:**

*Constants.py*
- Contains the strings used for multiple python scripts, including command configurations
  
*Benchmark.py*
- python benchmark.py
- Executes the workload configuration (mostly found on constants.py) for multiple models, hyperparameter settings
- Profiles the resource consumption parameters configured and saves the data on the /data/<GPU_model>_<dataset> directory
- Also saves the graph of each model being utilized in the workloads in its .pbtxt format
  
*Get_execution_time.py*
- python get_execution_time.py
- Obtains the execution time from data generated by benchmark.py with min-max scaling

*Generate_perf_file.py*

- python generate_perf_file.py
- Creates the perf file based on the results from benchmark.py and get_execution_time.py to create the format to prepare for Driple training
- The implementation of idle,burst, and peak parameters are defined in this script file.
  
*Node_frequency_per_gpu.py*

- python node_frequency_per_gpu.py
- Obtains the node frequency of the existing data and generates the node_frequency used for Driple and operation_count for analysis on the number of operations per model

**Training driple model:**

- Confirm location of node_frequency and perf_file to be in /driple/dataset/dataset_builder
- Run dataset builder: (based on https://github.com/gsyang33/Driple)
- python3 dataset_builder/generate_dataset.py --perf_result=[Result].csv --batch_size=32 --num_of_groups=100 --num_of_graphs=320 --save_path=[Path] --dataset_name=[Dataset].pkl
  *Using the generated dataset, train the new model using:*

**w/o TL:**
python ./driple/training/gcn.py --variable --gru --epochs=100000 --patience=1000 --variable_conv_layers=Nover2 --only_graph --hidden=64 --mlp_layers=3 --fixed 

**w/ TL (recommended):**
python ./driple/training/gcn.py --variable --gru --epochs=100000 --patience=1000 --variable_conv_layers=Nover2 --only_graph --hidden=64 --mlp_layers=3 --fixed --pre_trained=training/pre-train.pkl --data=[Dataset].pkl --transfer

*Using the trained model use the .pkl file and adjust configurations accordingly (w/ or w/o TL) and run:*

**Prediction and Allocation Score**
- Predict_driple_model.py
- python predict_driple_model.py
- Loads the input workload and generates a prediction score based on efficiency and execution time for job allocation

*parsing.py and grouping.py are extracted from the /driple directory to the base directory to be used by predict_driple_model.py for loading inputs*
